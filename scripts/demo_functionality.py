#!/usr/bin/env python3
"""
DeerFlow Functionality Demo Script
Demonstrates core features and architectural components of the project
"""

import sys
import os
import asyncio
from typing import Dict, Any

sys.path.insert(0, os.path.abspath('.'))

def demo_config_system():
    """Demonstrate configuration system"""
    print("\n=== Configuration System Demo ===")
    
    from src.config.config_loader import ConfigLoader
    
    config_loader = ConfigLoader()
    config_data = config_loader.load_config()
    
    print(f"âœ“ Configuration file loaded successfully")
    print(f"  - Basic model: {config_data.get('BASIC_MODEL', {}).get('model', 'Unknown')}")
    print(f"  - Max search results: {config_data.get('max_search_results', 3)}")
    print(f"  - Parallel execution: {config_data.get('PARALLEL_EXECUTION', {}).get('enable_parallel_execution', False)}")
    
    # Create complete configuration object
    configuration = config_loader.create_configuration()
    print(f"  - Configuration object created successfully with {len(configuration.model_token_limits)} model limits")

async def demo_memory_management():
    """Demonstrate memory management system"""
    print("\n=== Memory Management System Demo ===")
    
    from src.utils.performance.memory_manager import HierarchicalMemoryManager
    
    memory_manager = HierarchicalMemoryManager()
    
    # Test different levels of caching
    test_data = {
        "research_result_1": "This is an important research result",
        "search_cache_1": "Search result cache data",
        "temp_data_1": "Temporary processing data"
    }
    
    print("âœ“ Cache data to different levels:")
    for key, value in test_data.items():
        await memory_manager.set(key, value)
        print(f"  - Cached {key}: {len(value)} characters")
    
    # Test cache retrieval
    print("\nâœ“ Cache retrieval test:")
    for key in test_data.keys():
        cached_value = await memory_manager.get(key)
        if cached_value:
            print(f"  - Retrieved {key}: Success")
        else:
            print(f"  - Retrieved {key}: Failed")
    
    # Display cache statistics
    stats = memory_manager.get_stats()
    print(f"\nâœ“ Cache statistics:")
    print(f"  - L1 cache: {stats.get('l1_entries', 0)} entries")
    print(f"  - L2 cache: {stats.get('l2_entries', 0)} entries")
    print(f"  - L3 cache: {stats.get('l3_entries', 0)} entries")
    print(f"  - Cache hit rate: {stats.get('hit_rate', 0):.2%}")
    print(f"  - L1 utilization: {stats.get('l1_utilization', 0):.2%}")

def demo_content_processing():
    """Demonstrate content processing system"""
    print("\n=== Content Processing System Demo ===")
    
    from src.utils.tokens.content_processor import ContentProcessor
    
    processor = ContentProcessor()
    
    # Test texts
    test_texts = [
        "This is a simple test text.",
        "Artificial Intelligence (AI) is a branch of computer science that attempts to understand the essence of intelligence and produce a new intelligent machine that can react in a way similar to human intelligence.",
        "DeerFlow is a powerful AI research framework that supports multi-agent collaboration and deep research capabilities. It uses a Python backend and React frontend, providing a complete research workflow."
    ]
    
    print("âœ“ Token estimation test:")
    for i, text in enumerate(test_texts, 1):
        tokens = processor.estimate_tokens(text)
        print(f"  - Text {i}: {tokens} tokens ({len(text)} characters)")
    
    print("\nâœ“ Content cleaning test:")
    dirty_text = "<script>alert('test')</script>This is a text containing HTML tags<div>content</div>"
    clean_text = processor.sanitize_content(dirty_text)
    print(f"  - Original: {dirty_text}")
    print(f"  - Cleaned: {clean_text}")
    
    print("\nâœ“ Smart chunking test:")
    long_text = "\n\n".join(test_texts * 3)  # Create longer text
    chunks = processor.smart_chunk_content(long_text, model_name="deepseek-chat", chunk_strategy="auto")
    print(f"  - Original text: {len(long_text)} characters")
    print(f"  - Chunking result: {len(chunks)} chunks")
    for i, chunk in enumerate(chunks[:3], 1):  # Only show first 3 chunks
        print(f"    Chunk {i}: {len(chunk)} characters")

async def demo_context_management():
    """Demonstrate context management system"""
    print("\n=== Context Management System Demo ===")
    
    from src.config.config_loader import ConfigLoader
    from src.utils.context.context_evaluator import ContextStateEvaluator
    from src.utils.context.execution_context_manager import ExecutionContextManager
    
    # Load configuration
    config_loader = ConfigLoader()
    configuration = config_loader.create_configuration()
    
    # Context evaluator
    evaluator = ContextStateEvaluator(configuration)
    print("âœ“ Context evaluator initialized successfully")
    
    # Execution context manager
    context_manager = ExecutionContextManager()
    print("âœ“ Execution context manager initialized successfully")
    
    # Mock context data
    mock_context = {
        "current_task": "Research the development history of artificial intelligence",
        "previous_results": ["AI originated in the 1950s", "Deep learning emerged in the 2010s"],
        "search_results": ["Related paper 1", "Related paper 2", "Related paper 3"],
        "user_query": "What is artificial intelligence?"
    }
    
    print("\nâœ“ Context optimization test:")
    print(f"  - Original context items: {len(mock_context)}")
    print(f"  - Current task: {mock_context['current_task']}")
    print(f"  - Historical results: {len(mock_context['previous_results'])} items")
    print(f"  - Search results: {len(mock_context['search_results'])} items")

def demo_workflow_optimizer():
    """Demonstrate workflow optimizer"""
    print("\n=== Workflow Optimizer Demo ===")
    
    from src.utils.performance.workflow_optimizer import WorkflowOptimizer
    
    optimizer = WorkflowOptimizer()
    print("âœ“ Workflow optimizer initialized successfully")
    
    # Mock workflow steps
    workflow_steps = [
        {"name": "query_analysis", "duration": 0.5, "tokens": 100},
        {"name": "search_execution", "duration": 2.0, "tokens": 500},
        {"name": "content_processing", "duration": 1.5, "tokens": 800},
        {"name": "result_synthesis", "duration": 1.0, "tokens": 300}
    ]
    
    print("\nâœ“ Workflow step analysis:")
    total_duration = sum(step["duration"] for step in workflow_steps)
    total_tokens = sum(step["tokens"] for step in workflow_steps)
    
    for step in workflow_steps:
        print(f"  - {step['name']}: {step['duration']}s, {step['tokens']} tokens")
    
    print(f"\nâœ“ Workflow statistics:")
    print(f"  - Total duration: {total_duration}s")
    print(f"  - Total tokens: {total_tokens}")
    print(f"  - Average step duration: {total_duration/len(workflow_steps):.2f}s")

def demo_error_handling():
    """Demonstrate error handling system"""
    print("\n=== Error Handling System Demo ===")
    
    from src.llms.error_handler import LLMErrorHandler
    
    error_handler = LLMErrorHandler()
    print("âœ“ Error handler initialized successfully")
    
    # Mock different types of errors
    test_errors = [
        "Rate limit exceeded",
        "Token limit exceeded",
        "API key invalid",
        "Network timeout",
        "Unknown error occurred"
    ]
    
    print("\nâœ“ Error classification test:")
    for error_msg in test_errors:
        error_type = error_handler.classify_error(error_msg)
        should_retry = error_handler.should_retry_error(error_type)
        print(f"  - '{error_msg}' -> {error_type} (Retry: {'Yes' if should_retry else 'No'})")

async def demo_graph_workflow():
    """Demonstrate graph workflow"""
    print("\n=== Graph Workflow Demo ===")
    
    try:
        from src.workflow import graph
        print("âœ“ Workflow graph loaded successfully")
        
        # Get basic graph information
        if hasattr(graph, 'get_graph'):
            graph_info = graph.get_graph()
            if hasattr(graph_info, 'nodes'):
                print(f"  - Graph nodes: {len(graph_info.nodes)}")
            if hasattr(graph_info, 'edges'):
                print(f"  - Graph edges: {len(graph_info.edges)}")
        
        print("  - Workflow graph structure is complete")
        
    except Exception as e:
        print(f"âœ— Workflow graph loading failed: {e}")

async def main():
    """Main demonstration function"""
    print("DeerFlow Functionality Demo")
    print("=" * 60)
    
    # Run various demonstrations
    demo_config_system()
    await demo_memory_management()
    demo_content_processing()
    await demo_context_management()
    demo_workflow_optimizer()
    demo_error_handling()
    await demo_graph_workflow()
    
    print("\n" + "=" * 60)
    print("ðŸŽ‰ DeerFlow functionality demo completed!")
    print("\nCore functionality summary:")
    print("âœ“ Configuration management system - Flexible YAML configuration loading")
    print("âœ“ Memory management system - Three-tier cache architecture")
    print("âœ“ Content processing system - Smart chunking and token management")
    print("âœ“ Context management system - Dynamic context optimization")
    print("âœ“ Workflow optimizer - Performance monitoring and optimization")
    print("âœ“ Error handling system - Smart error classification and retry")
    print("âœ“ Graph workflow system - LangGraph multi-agent collaboration")
    
    print("\nProject is ready for use!")
    print("\nUsage instructions:")
    print("1. Configure API keys in conf.yaml file")
    print("2. Run 'uv run python main.py' to start command line interface")
    print("3. Run 'uv run python server.py' to start web service")
    print("4. Access web/ directory to start frontend interface")

if __name__ == "__main__":
    asyncio.run(main())
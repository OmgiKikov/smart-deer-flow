# New Configuration Format for DeerFlow
# This file uses the new Pydantic-based configuration system

# Core settings
report_style: "academic"  # Options: academic, business, technical, casual
resources: []  # List of custom resources

# LLM Configuration
llm:
  temperature: 0.7
  timeout: 30
  max_tokens: null  # Use model default

# Agent Configuration
agents:
  max_plan_iterations: 1
  max_step_num: 5
  max_search_results: 5
  enable_deep_thinking: false
  enable_parallel_execution: true
  max_parallel_tasks: 3
  max_context_steps_parallel: 2
  disable_context_parallel: false

# Research Configuration
research:
  enable_researcher_isolation: true
  researcher_isolation_level: "moderate"  # Options: minimal, moderate, aggressive
  researcher_max_local_context: 5000
  researcher_isolation_threshold: 0.7
  researcher_auto_isolation: false
  researcher_isolation_metrics: false
  max_context_steps_researcher: 2

# Reflection Configuration
reflection:
  enable_enhanced_reflection: true
  max_reflection_loops: 3
  reflection_temperature: 0.7
  reflection_trigger_threshold: 2
  reflection_confidence_threshold: 0.7
  enable_reflection_integration: true
  enable_progressive_reflection: true
  enable_reflection_metrics: true

# Iterative Research Configuration
iterative_research:
  max_follow_up_iterations: 3
  sufficiency_threshold: 0.7
  enable_iterative_research: true
  max_queries_per_iteration: 3
  follow_up_delay_seconds: 1.0

# Content Processing Configuration
content:
  enable_content_summarization: true
  enable_smart_filtering: true
  summary_type: "comprehensive"  # Options: comprehensive, key_points, abstract

# Advanced Context Management
advanced_context:
  max_context_ratio: 0.6
  sliding_window_size: 5
  overlap_ratio: 0.2
  compression_threshold: 0.8
  default_strategy: "adaptive"
  priority_weights:
    critical: 1.0
    high: 0.7
    medium: 0.4
    low: 0.1
  enable_caching: true
  enable_analytics: true
  debug_mode: false

# MCP Configuration
mcp:
  enabled: false
  servers: []
  timeout: 30

# Model Token Limits
model_token_limits:
  deepseek-chat:
    input_limit: 65536
    output_limit: 8192
    context_window: 131072
    safety_margin: 0.2
  deepseek-reasoner:
    input_limit: 65536
    output_limit: 8192
    context_window: 131072
    safety_margin: 0.2
  gpt-4:
    input_limit: 32768
    output_limit: 4096
    context_window: 32768
    safety_margin: 0.8
  claude-3-sonnet:
    input_limit: 200000
    output_limit: 4096
    context_window: 200000
    safety_margin: 0.8

# Search Engine Configuration
search:
  engine: "tavily"  # Options: tavily, duckduckgo, brave
  timeout: 30
  max_retries: 3

# Logging Configuration
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: null  # Set to log file path for file logging

# Performance Configuration
performance:
  enable_caching: true
  cache_size: 1000
  cache_ttl: 3600  # seconds
  max_workers: 4
  timeout: 300  # seconds
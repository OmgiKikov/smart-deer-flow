# [!NOTE]
# Read the `docs/configuration_guide.md` carefully, and update the
# configurations to match your specific settings and requirements.
# - Replace `api_key` with your own credentials.
# - Replace `base_url` and `model` name if you want to use a custom model.
# - Set `verify_ssl` to `false` if your LLM server uses self-signed certificates
# - A restart is required every time you change the `config.yaml` file.

BASIC_MODEL:
  base_url: https://ark.cn-beijing.volces.com/api/v3
  model: "doubao-1-5-pro-32k-250115"
  api_key: xxxx
  # verify_ssl: false  # Uncomment this line to disable SSL certificate verification for self-signed certificates
  
  # Token limit configuration (for smart content processing)
  token_limits:
    input_limit: 32000      # Input token limit
    output_limit: 4096      # Output token limit
    context_window: 32000   # Context window size
    safety_margin: 0.8      # Safety margin (actual usage limit = limit * safety margin)
 
# Reasoning model is optional.
# Uncomment the following settings if you want to use reasoning model
# for planning.

# REASONING_MODEL:
#   base_url: https://ark-cn-beijing.bytedance.net/api/v3
#   model: "doubao-1-5-thinking-pro-m-250428"
#   api_key: xxxx
#   # Token limit configuration (optional)
#   token_limits:
#     input_limit: 32000
#     output_limit: 8192
#     context_window: 32000
#     safety_margin: 0.8

# Smart content processing configuration
# Used to solve large model token limit issues
CONTENT_PROCESSING:
  enable_smart_chunking: true          # Enable smart chunking
  enable_content_summarization: true   # Enable content summarization
  enable_smart_filtering: true         # Enable LLM-based search result filtering
  chunk_strategy: "auto"               # Chunking strategy: auto, sentences, paragraphs
  summary_type: "comprehensive"        # Summary type: comprehensive, key_points, abstract

# Search configuration
max_search_results: 3  # Maximum number of search results

# Parallel execution configuration
PARALLEL_EXECUTION:
  enable_parallel_execution: true      # Enable parallel execution
  max_context_steps_parallel: 1        # Maximum context steps in parallel (reduced for token optimization)
  disable_context_parallel: false      # Disable context sharing in parallel execution

# Advanced context management configuration
# These settings optimize token usage and prevent context length exceeded errors
ADVANCED_CONTEXT_MANAGEMENT:
  max_context_ratio: 0.6               # Use 60% of model limit for context
  sliding_window_size: 5               # Number of recent interactions to keep
  overlap_ratio: 0.2                   # Overlap between sliding windows (0.0-1.0)
  compression_threshold: 0.8           # Trigger compression at 80% token capacity
  default_strategy: "adaptive"         # Default compression strategy: adaptive, hierarchical, sliding_window, summarize, truncate, none
  
  # Priority weights for content importance
  priority_weights:
    critical: 1.0                      # System instructions, current task
    high: 0.7                          # Recent interactions, key decisions
    medium: 0.4                        # Historical context, background info
    low: 0.1                           # Auxiliary information
  
  # Performance optimization settings
  enable_caching: true                 # Cache compressed content for reuse
  enable_analytics: true               # Track optimization statistics
  debug_mode: false                    # Enable detailed debug logging
